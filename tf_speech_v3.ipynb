{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. import trainning sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/media/share/data/kaggle/tensorflow-speech/'\n",
    "train_path = filepath + 'train/audio/'\n",
    "test_path = filepath + 'test/audio/'\n",
    "\n",
    "target_labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make training list\n",
    "import fnmatch\n",
    "\n",
    "train_df = pd.DataFrame([], columns=['fname', 'label'])\n",
    "labels = os.listdir(train_path)\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    labelpath = train_path + label\n",
    "    filelist = fnmatch.filter(os.listdir(labelpath), '*.wav')\n",
    "    filelist = pd.DataFrame(filelist, columns=['fname'])\n",
    "    if label == '_background_noise_':\n",
    "        filelist['label'] = 'silence'\n",
    "    elif label not in target_labels:\n",
    "        filelist['label'] = 'unknown'\n",
    "    else:\n",
    "        filelist['label'] = label\n",
    "    filelist['path'] = label\n",
    "    \n",
    "    train_df = pd.concat([train_df, filelist], 0)\n",
    "    \n",
    "# random order\n",
    "train_df = train_df.sample(frac=1).reset_index(drop='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=25,\n",
    "                 step_size=15, eps=1e-8):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "new_sample_rate = 8000\n",
    "\n",
    "X1 = []\n",
    "y1 = []\n",
    "\n",
    "for j, fname in enumerate(train_df['fname']):\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_path+train_df['path'][j], fname))\n",
    "\n",
    "    if len(samples) > 16000:\n",
    "        new_clip = np.random.randint(100, len(samples) - 16000)\n",
    "        samples = samples[new_clip : (new_clip + 16000)]\n",
    "    else:\n",
    "        samples = np.pad(samples, (0, 16000-len(samples)), 'constant')\n",
    "\n",
    "    resampled = signal.resample(samples, int(new_sample_rate/sample_rate * samples.shape[0]))\n",
    "    freqs, times, spectrogram = log_specgram(resampled, new_sample_rate)\n",
    "    norm_spect = StandardScaler().fit_transform(spectrogram)\n",
    "\n",
    "    X1.append(norm_spect)\n",
    "    y1.append(train_df['label'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(X1)\n",
    "X1 = X1.reshape(tuple(list(X1.shape) + [1]))\n",
    "y1 = pd.get_dummies(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 101, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 98, 101, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 98, 101, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 98, 101, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 98, 101, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 98, 101, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 98, 101, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 98, 101, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 98, 101, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 98, 101, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 12)           4980208     lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Concatenate)           (None, 12)           0           model_3[1][0]                    \n",
      "                                                                 model_3[2][0]                    \n",
      "                                                                 model_3[3][0]                    \n",
      "                                                                 model_3[4][0]                    \n",
      "                                                                 model_3[5][0]                    \n",
      "                                                                 model_3[6][0]                    \n",
      "                                                                 model_3[7][0]                    \n",
      "                                                                 model_3[8][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,980,208\n",
      "Trainable params: 4,942,158\n",
      "Non-trainable params: 38,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import Concatenate, Add, concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "input_shape = (98, 101, 1)\n",
    "nclass = 12\n",
    "\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Conv2D(16, kernel_size, activation='relu', padding='same')(norm_inp)\n",
    "img_1 = Conv2D(32, kernel_size, activation='relu', padding='same')(BatchNormalization()(img_1))\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.1)(img_1)\n",
    "img_1 = Conv2D(32, kernel_size, activation='relu', padding='same')(BatchNormalization()(img_1))\n",
    "img_1 = Conv2D(64, kernel_size, activation='relu', padding='same')(BatchNormalization()(img_1))\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.15)(img_1)\n",
    "img_1 = Conv2D(64, kernel_size, activation='relu', padding='same')(BatchNormalization()(img_1))\n",
    "img_1 = Conv2D(128, kernel_size, activation='relu', padding='same')(BatchNormalization()(img_1))\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = Dense(256, activation='relu')(BatchNormalization()(img_1))\n",
    "dense_1 = Dense(128, activation='relu')(BatchNormalization()(dense_1))\n",
    "dense_1 = Dense(64, activation='relu')(BatchNormalization()(dense_1))\n",
    "dense_1 = Dense(nclass, activation='softmax')(dense_1)\n",
    "\n",
    "model = Model(inputs=inp, outputs=dense_1)\n",
    "\n",
    "multi_gpu = multi_gpu_model(model, gpus=8)\n",
    "\n",
    "multi_gpu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpu.compile(optimizer=Adam(lr=0.002), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "os.chdir('/media/share/jiaxin_cmu/kaggle/TF_speech/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau\n",
    "import datetime\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('TF_speech_v2-{epoch:02d}-{val_loss:.4f}.hdf5',\n",
    "                                   monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "adlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=0, \n",
    "                         mode='auto', epsilon=0.0001, cooldown=0, min_lr=1e-6)\n",
    "\n",
    "train_datagen = ImageDataGenerator(width_shift_range = 0.30, \n",
    "                                   height_shift_range = 0.30, \n",
    "                                   zoom_range = 0.15)\n",
    "\n",
    "test_datagen = ImageDataGenerator(width_shift_range = 0.05)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X1, y1, test_size=0.1, random_state=np.random)\n",
    "\n",
    "train_datagen.fit(x_train)\n",
    "test_datagen.fit(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3859 - acc: 0.9438 - val_loss: 0.0177 - val_acc: 0.9958\n",
      "Epoch 2/64\n",
      "56/56 [==============================] - 100s 2s/step - loss: 0.3813 - acc: 0.9440 - val_loss: 0.0160 - val_acc: 0.9958\n",
      "Epoch 3/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3922 - acc: 0.9425 - val_loss: 0.0147 - val_acc: 0.9961\n",
      "Epoch 4/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3822 - acc: 0.9438 - val_loss: 0.0152 - val_acc: 0.9961\n",
      "Epoch 5/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3821 - acc: 0.9433 - val_loss: 0.0143 - val_acc: 0.9963\n",
      "Epoch 6/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3963 - acc: 0.9419 - val_loss: 0.0160 - val_acc: 0.9960\n",
      "Epoch 7/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3950 - acc: 0.9413 - val_loss: 0.0155 - val_acc: 0.9957\n",
      "Epoch 8/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3797 - acc: 0.9429 - val_loss: 0.0167 - val_acc: 0.9959\n",
      "Epoch 9/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3763 - acc: 0.9433 - val_loss: 0.0151 - val_acc: 0.9961\n",
      "Epoch 10/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3824 - acc: 0.9427 - val_loss: 0.0152 - val_acc: 0.9963\n",
      "Epoch 11/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3738 - acc: 0.9439 - val_loss: 0.0156 - val_acc: 0.9957\n",
      "Epoch 12/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3709 - acc: 0.9438 - val_loss: 0.0152 - val_acc: 0.9963\n",
      "Epoch 13/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3809 - acc: 0.9424 - val_loss: 0.0147 - val_acc: 0.9964\n",
      "Epoch 14/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3701 - acc: 0.9436 - val_loss: 0.0145 - val_acc: 0.9964\n",
      "Epoch 15/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3816 - acc: 0.9426 - val_loss: 0.0161 - val_acc: 0.9957\n",
      "Epoch 16/64\n",
      "56/56 [==============================] - 100s 2s/step - loss: 0.3747 - acc: 0.9430 - val_loss: 0.0171 - val_acc: 0.9957\n",
      "Epoch 17/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3745 - acc: 0.9427 - val_loss: 0.0143 - val_acc: 0.9964\n",
      "Epoch 18/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3664 - acc: 0.9434 - val_loss: 0.0140 - val_acc: 0.9963\n",
      "Epoch 19/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3649 - acc: 0.9431 - val_loss: 0.0158 - val_acc: 0.9960\n",
      "Epoch 20/64\n",
      "56/56 [==============================] - 105s 2s/step - loss: 0.3649 - acc: 0.9436 - val_loss: 0.0146 - val_acc: 0.9961\n",
      "Epoch 21/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3667 - acc: 0.9432 - val_loss: 0.0152 - val_acc: 0.9961\n",
      "Epoch 22/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3713 - acc: 0.9423 - val_loss: 0.0163 - val_acc: 0.9957\n",
      "Epoch 23/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3708 - acc: 0.9418 - val_loss: 0.0161 - val_acc: 0.9957\n",
      "Epoch 24/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3673 - acc: 0.9425 - val_loss: 0.0160 - val_acc: 0.9962\n",
      "Epoch 25/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3689 - acc: 0.9424 - val_loss: 0.0163 - val_acc: 0.9959\n",
      "Epoch 26/64\n",
      "56/56 [==============================] - 105s 2s/step - loss: 0.3612 - acc: 0.9430 - val_loss: 0.0136 - val_acc: 0.9961\n",
      "Epoch 27/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3610 - acc: 0.9428 - val_loss: 0.0171 - val_acc: 0.9957\n",
      "Epoch 28/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3640 - acc: 0.9425 - val_loss: 0.0148 - val_acc: 0.9961\n",
      "Epoch 29/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3626 - acc: 0.9425 - val_loss: 0.0160 - val_acc: 0.9963\n",
      "Epoch 30/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3684 - acc: 0.9418 - val_loss: 0.0144 - val_acc: 0.9961\n",
      "Epoch 31/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3610 - acc: 0.9426 - val_loss: 0.0142 - val_acc: 0.9962\n",
      "Epoch 32/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3576 - acc: 0.9429 - val_loss: 0.0163 - val_acc: 0.9958\n",
      "Epoch 33/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3538 - acc: 0.9433 - val_loss: 0.0137 - val_acc: 0.9964\n",
      "Epoch 34/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3540 - acc: 0.9432 - val_loss: 0.0156 - val_acc: 0.9960\n",
      "Epoch 35/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3507 - acc: 0.9433 - val_loss: 0.0164 - val_acc: 0.9961\n",
      "Epoch 36/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3500 - acc: 0.9438 - val_loss: 0.0160 - val_acc: 0.9955\n",
      "Epoch 37/64\n",
      "56/56 [==============================] - 100s 2s/step - loss: 0.3512 - acc: 0.9433 - val_loss: 0.0146 - val_acc: 0.9963\n",
      "Epoch 38/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3543 - acc: 0.9427 - val_loss: 0.0156 - val_acc: 0.9958\n",
      "Epoch 39/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3410 - acc: 0.9441 - val_loss: 0.0155 - val_acc: 0.9959\n",
      "Epoch 40/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3430 - acc: 0.9437 - val_loss: 0.0166 - val_acc: 0.9957\n",
      "Epoch 41/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3483 - acc: 0.9435 - val_loss: 0.0179 - val_acc: 0.9952\n",
      "Epoch 42/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3496 - acc: 0.9434 - val_loss: 0.0146 - val_acc: 0.9959\n",
      "Epoch 43/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3454 - acc: 0.9434 - val_loss: 0.0150 - val_acc: 0.9958\n",
      "Epoch 44/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3500 - acc: 0.9429 - val_loss: 0.0155 - val_acc: 0.9957\n",
      "Epoch 45/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3459 - acc: 0.9434 - val_loss: 0.0164 - val_acc: 0.9958\n",
      "Epoch 46/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3364 - acc: 0.9441 - val_loss: 0.0139 - val_acc: 0.9963\n",
      "Epoch 47/64\n",
      "56/56 [==============================] - 104s 2s/step - loss: 0.3384 - acc: 0.9444 - val_loss: 0.0136 - val_acc: 0.9962\n",
      "Epoch 48/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3309 - acc: 0.9447 - val_loss: 0.0149 - val_acc: 0.9962\n",
      "Epoch 49/64\n",
      "56/56 [==============================] - 106s 2s/step - loss: 0.3362 - acc: 0.9438 - val_loss: 0.0134 - val_acc: 0.9964\n",
      "Epoch 50/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3456 - acc: 0.9423 - val_loss: 0.0135 - val_acc: 0.9966\n",
      "Epoch 51/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3392 - acc: 0.9435 - val_loss: 0.0144 - val_acc: 0.9963\n",
      "Epoch 52/64\n",
      "56/56 [==============================] - 100s 2s/step - loss: 0.3414 - acc: 0.9433 - val_loss: 0.0142 - val_acc: 0.9963\n",
      "Epoch 53/64\n",
      "56/56 [==============================] - 98s 2s/step - loss: 0.3361 - acc: 0.9436 - val_loss: 0.0163 - val_acc: 0.9957\n",
      "Epoch 54/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3371 - acc: 0.9432 - val_loss: 0.0154 - val_acc: 0.9963\n",
      "Epoch 55/64\n",
      "56/56 [==============================] - 100s 2s/step - loss: 0.3334 - acc: 0.9440 - val_loss: 0.0170 - val_acc: 0.9961\n",
      "Epoch 56/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3409 - acc: 0.9427 - val_loss: 0.0165 - val_acc: 0.9957\n",
      "Epoch 57/64\n",
      "56/56 [==============================] - 100s 2s/step - loss: 0.3331 - acc: 0.9434 - val_loss: 0.0156 - val_acc: 0.9958\n",
      "Epoch 58/64\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.3253 - acc: 0.9447 - val_loss: 0.0152 - val_acc: 0.9959\n",
      "Epoch 59/64\n",
      "56/56 [==============================] - 101s 2s/step - loss: 0.3302 - acc: 0.9438 - val_loss: 0.0159 - val_acc: 0.9961\n",
      "Epoch 60/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3284 - acc: 0.9437 - val_loss: 0.0149 - val_acc: 0.9956\n",
      "Epoch 61/64\n",
      "56/56 [==============================] - 100s 2s/step - loss: 0.3348 - acc: 0.9433 - val_loss: 0.0158 - val_acc: 0.9958\n",
      "Epoch 62/64\n",
      "56/56 [==============================] - 102s 2s/step - loss: 0.3366 - acc: 0.9431 - val_loss: 0.0135 - val_acc: 0.9963\n",
      "Epoch 63/64\n",
      "56/56 [==============================] - 100s 2s/step - loss: 0.3359 - acc: 0.9429 - val_loss: 0.0155 - val_acc: 0.9957\n",
      "Epoch 64/64\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.3272 - acc: 0.9438 - val_loss: 0.0140 - val_acc: 0.9961\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_history = multi_gpu.fit_generator(train_datagen.flow(x_train, y_train, batch_size), \n",
    "                                        epochs=2**6, steps_per_epoch=(len(y_train)//batch_size), \n",
    "                                        validation_data=test_datagen.flow(x_valid, y_valid, batch_size), \n",
    "                                        validation_steps=(len(x_valid)//batch_size),\n",
    "                                        verbose=1, callbacks=[model_checkpoint, adlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6473/6473 [==============================] - 9s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0047744662503417584, 0.99893145279079176]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_weights('statoilv5-52-0.1685.hdf5')\n",
    "multi_gpu.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "submpath = '/media/share/jiaxin_cmu/kaggle/TF_speech/'\n",
    "subm_df = pd.read_csv(filepath + 'sample_submission.csv')\n",
    "\n",
    "# new_sample_rate = 8000\n",
    "# test_X = []\n",
    "\n",
    "# for j, fname in enumerate(subm_df['fname']):\n",
    "#     sample_rate, samples = wavfile.read(os.path.join(test_path, fname))\n",
    "\n",
    "#     if len(samples) > 16000:\n",
    "#         new_clip = np.random.randint(0, len(samples) - 16000)\n",
    "#         samples = samples[new_clip : (new_clip + 16000)]\n",
    "#     else:\n",
    "#         samples = np.pad(samples, (0, 16000-len(samples)), 'constant')\n",
    "\n",
    "#     resampled = signal.resample(samples, int(new_sample_rate/sample_rate * samples.shape[0]))\n",
    "#     freqs, times, spectrogram = log_specgram(resampled, new_sample_rate)\n",
    "#     norm_spect = StandardScaler().fit_transform(spectrogram)\n",
    "\n",
    "#     test_X.append(norm_spect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array(test_X)\n",
    "test_X = test_X.reshape(tuple(list(test_X.shape) + [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = multi_gpu.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = np.argmax(pred_y, axis=1)\n",
    "predicts = [target_labels[p] for p in predicts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df['label'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clip_000044442.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clip_0000adecb.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clip_0000d4322.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clip_0000fb6fe.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clip_0001d1559.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fname    label\n",
       "0  clip_000044442.wav  unknown\n",
       "1  clip_0000adecb.wav  unknown\n",
       "2  clip_0000d4322.wav  unknown\n",
       "3  clip_0000fb6fe.wav  unknown\n",
       "4  clip_0001d1559.wav  unknown"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clip_000044442.wav</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clip_0000adecb.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clip_0000d4322.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clip_0000fb6fe.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clip_0001d1559.wav</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fname label\n",
       "0  clip_000044442.wav  down\n",
       "1  clip_0000adecb.wav    go\n",
       "2  clip_0000d4322.wav    go\n",
       "3  clip_0000fb6fe.wav    go\n",
       "4  clip_0001d1559.wav    go"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_df.to_csv(submpath + 'submission_unknow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
